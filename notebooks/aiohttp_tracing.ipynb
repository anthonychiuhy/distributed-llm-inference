{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1a786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3d5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://10.215.130.20:11434/api/generate\"\n",
    "PAYLOAD = {\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": \"Write a short poem about AI and Telecoms\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200,\n",
    "    \"stream\": False \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e01c6727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request start 2026-01-01 21:18:24.100286\n",
      "request sent 2026-01-01 21:18:27.654367\n",
      "{'model': 'mistral', 'created_at': '2026-01-01T21:18:27.615698289Z', 'response': ' Hello! How can I assist you today? If you have any questions or need help with something, feel free to ask.\\n\\nHere are a few things I can help you with:\\n\\n1. Answering factual questions (e.g., \"What is the capital of France?\")\\n2. Explaining concepts and providing information on various topics (e.g., science, history, literature, etc.)\\n3. Helping with math problems (e.g., solving equations, finding factors, etc.)\\n4. Generating creative content (e.g., writing short stories, composing poetry, etc.)\\n5. Offering advice and suggestions based on the information you provide\\n6. Assisting with language learning by providing translations, explaining grammar rules, helping with vocabulary, etc.\\n7. Providing fun and interesting facts, trivia, jokes, and more!\\n\\nIf there\\'s something else you need help with or if you have a specific question in mind, please let me know! I\\'m here to make your life easier.', 'done': True, 'done_reason': 'stop', 'context': [3, 29473, 23325, 4, 29473, 23325, 29576, 2370, 1309, 1083, 6799, 1136, 3922, 29572, 1815, 1136, 1274, 1475, 4992, 1210, 1695, 2084, 1163, 2313, 29493, 2369, 2701, 1066, 2228, 29491, 781, 781, 16191, 1228, 1032, 2432, 2490, 1083, 1309, 2084, 1136, 1163, 29515, 781, 781, 29508, 29491, 1862, 2384, 2899, 2407, 1608, 4992, 1093, 29474, 29491, 29489, 2831, 1113, 3963, 1117, 1040, 6333, 1070, 5611, 1878, 29499, 781, 29518, 29491, 14470, 2549, 17350, 1072, 8269, 2639, 1124, 4886, 14585, 1093, 29474, 29491, 29489, 2831, 7459, 29493, 4108, 29493, 12122, 29493, 5113, 3742, 781, 29538, 29491, 6192, 3685, 1163, 11817, 5186, 1093, 29474, 29491, 29489, 2831, 22868, 13269, 29493, 8252, 9380, 29493, 5113, 3742, 781, 29549, 29491, 3534, 1845, 10579, 3804, 1093, 29474, 29491, 29489, 2831, 4421, 3253, 6747, 29493, 6658, 1056, 16659, 29493, 5113, 3742, 781, 29550, 29491, 4752, 2899, 8246, 1072, 18046, 3586, 1124, 1040, 2639, 1136, 3852, 781, 29552, 29491, 4116, 10905, 1163, 4610, 5936, 1254, 8269, 8022, 1465, 29493, 21168, 19524, 4247, 6647, 29493, 9306, 1163, 8669, 24768, 29493, 5113, 29491, 781, 29555, 29491, 7901, 4037, 1514, 1072, 6621, 12180, 29493, 1029, 1582, 1283, 29493, 24640, 29493, 1072, 1448, 29576, 781, 781, 4149, 1504, 29510, 29481, 2313, 1880, 1136, 1695, 2084, 1163, 1210, 1281, 1136, 1274, 1032, 3716, 3764, 1065, 3041, 29493, 5433, 2114, 1296, 1641, 29576, 1083, 29510, 29487, 2004, 1066, 1806, 1342, 2179, 7857, 29491], 'total_duration': 3505922429, 'load_duration': 990228919, 'prompt_eval_count': 5, 'prompt_eval_duration': 85928647, 'eval_count': 226, 'eval_duration': 2429119693}\n",
      "response_complete 2026-01-01 21:18:27.654600\n",
      "Request 0: Server received in 3554.31ms\n"
     ]
    }
   ],
   "source": [
    "class TimingTrace(aiohttp.TraceConfig):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.timings = {}\n",
    "        self.on_request_start.append(self.on_request_start_callback)\n",
    "        self.on_request_chunk_sent.append(self.on_request_chunk_sent_callback)\n",
    "        self.on_response_chunk_received.append(self.on_response_start_callback)\n",
    "    \n",
    "    async def on_request_start_callback(self, session, context, params):\n",
    "        context.request_start = datetime.now()\n",
    "        print(f\"Request started (client): {context.request_start}\")\n",
    "    \n",
    "    async def on_request_chunk_sent_callback(self, session, context, params):\n",
    "        context.request_sent = datetime.now()\n",
    "        print(f\"Request sent to server: {context.request_sent}\")\n",
    "    \n",
    "    async def on_response_start_callback(self, session, context, params):\n",
    "        context.server_received = datetime.now()\n",
    "        print(f\"Server acknowledged (first byte): {context.server_received}\")\n",
    "\n",
    "async def send_request_with_timing(request_id, prompt):\n",
    "    trace_config = TimingTrace()\n",
    "\n",
    "    url = API_URL\n",
    "    payload = {\n",
    "        \"model\": \"mistral\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession(trace_configs=[trace_config]) as session:\n",
    "        \n",
    "        client_send_time = datetime.now()\n",
    "\n",
    "        print(\"request start\", datetime.now())\n",
    "        \n",
    "        async with session.post(url, json=payload) as response:\n",
    "            # Server has received and started processing\n",
    "            server_ack_time = datetime.now()\n",
    "            print(\"request sent\", server_ack_time)\n",
    "            \n",
    "            data = await response.json()\n",
    "            print(data)\n",
    "            response_complete_time = datetime.now()\n",
    "            print(\"response_complete\", response_complete_time)\n",
    "            \n",
    "            return {\n",
    "                \"id\": request_id,\n",
    "                \"client_send\": client_send_time,\n",
    "                \"server_ack\": server_ack_time,  # This is when server received it\n",
    "                \"response_complete\": response_complete_time,\n",
    "                \"time_to_server\": (server_ack_time - client_send_time).total_seconds()\n",
    "            }\n",
    "\n",
    "async def traffic_generator(num_requests, prompt):\n",
    "    tasks = [send_request_with_timing(i, prompt) for i in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# Run\n",
    "results = await traffic_generator(1, \"Hello\")\n",
    "for r in results:\n",
    "    print(f\"Request {r['id']}: Server received in {r['time_to_server']*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e507c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request started: 2025-12-31 17:16:02.777273\n",
      "Also Request started (UTC): 2025-12-31 17:16:02.777868\n",
      "Request ended/ack received (UTC): 2025-12-31 17:16:02.817682\n",
      "Also received_at (UTC): 2025-12-31 17:16:02.817757\n",
      "Elapsed seconds: 0.03983436799899209\n",
      "Server Date: Wed, 31 Dec 2025 17:16:02 GMT\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "timings = {}\n",
    "\n",
    "async def on_request_start(session, ctx, params):\n",
    "    timings['start_wall'] = datetime.now()\n",
    "    print(\"Also Request started (UTC):\", timings.get('start_wall'))\n",
    "    timings['start_perf'] = time.perf_counter()\n",
    "\n",
    "async def on_request_end(session, ctx, params):\n",
    "    timings['end_wall'] = datetime.now()\n",
    "    print(\"Request ended/ack received (UTC):\", timings.get('end_wall'))\n",
    "    timings['end_perf'] = time.perf_counter()\n",
    "    timings['elapsed_sec'] = timings['end_perf'] - timings['start_perf']\n",
    "\n",
    "trace_config = aiohttp.TraceConfig()\n",
    "trace_config.on_request_start.append(on_request_start)\n",
    "trace_config.on_request_end.append(on_request_end)\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession(trace_configs=[trace_config]) as session:\n",
    "        start_at = datetime.now()\n",
    "        print(\"Request started:\", start_at)\n",
    "        async with session.post(API_URL, json={\"model\": \"llama2\", \"prompt\": \"Hello\", \"stream\": False}) as resp:\n",
    "            # This is when headers have been received\n",
    "            received_at = datetime.now()\n",
    "            print(\"Also received_at (UTC):\", received_at)\n",
    "            server_date_hdr = resp.headers.get(\"Date\")\n",
    "\n",
    "            \n",
    "            print(\"Elapsed seconds:\", timings.get('elapsed_sec'))\n",
    "            print(\"Server Date:\", server_date_hdr)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ccd232ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0 213314.905438168\n",
      "on_request_start 213314.905688437 {'a': 1, 'b': 2}\n",
      "on_request_headers_sent 213314.907520845\n",
      "on_request_chunk_sent 213314.907587289\n",
      "on_request_end 213314.971652295\n",
      "ID: 0 status 213314.971718502 200 <CIMultiDictProxy('Content-Type': 'application/x-ndjson', 'Date': 'Sat, 03 Jan 2026 01:58:35 GMT', 'Transfer-Encoding': 'chunked')>\n",
      "@@@\n",
      "b'{\"model\":\"mistral\",\"created_at\":\"2026-01-03T01:58:35.468028089Z\",\"response\":\" \",\"done\":false}\\n' True\n",
      "b'{\"model\":\"mistral\",\"created_at\":\"2026-01-03T01:58:35.479481193Z\",\"response\":\"0\",\"done\":false}\\n' True\n",
      "b'{\"model\":\"mistral\",\"created_at\":\"2026-01-03T01:58:35.490991874Z\",\"response\":\" tokens\",\"done\":false}\\n' True\n",
      "b'{\"model\":\"mistral\",\"created_at\":\"2026-01-03T01:58:35.502593167Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[3,29473,3957,29510,29475,10839,1163,3192,1206,1312,29491,13638,1346,3279,1206,1312,29493,10839,1163,29473,29502,17014,29491,4,1027,29502,17014],\"total_duration\":57222670,\"load_duration\":10164796,\"prompt_eval_count\":25,\"prompt_eval_duration\":2167122,\"eval_count\":4,\"eval_duration\":44448852}\\n' True\n",
      "b'' True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def on_request_start(session, ctx, params):\n",
    "    print(\"on_request_start\", time.perf_counter(), ctx.trace_request_ctx)\n",
    "\n",
    "async def on_request_headers_sent(session, ctx, params):\n",
    "    print(\"on_request_headers_sent\", time.perf_counter())\n",
    "\n",
    "async def on_request_chunk_sent(session, ctx, params):\n",
    "    print(\"on_request_chunk_sent\", time.perf_counter())\n",
    "\n",
    "async def on_request_end(session, ctx, params):\n",
    "    print(\"on_request_end\", time.perf_counter())\n",
    "\n",
    "async def on_response_chunk_received(session, ctx, params):\n",
    "    print(\"on_response_chunk_received\", time.perf_counter())\n",
    "\n",
    "\n",
    "\n",
    "trace_config = aiohttp.TraceConfig()\n",
    "trace_config.on_request_start.append(on_request_start)\n",
    "trace_config.on_request_headers_sent.append(on_request_headers_sent)\n",
    "trace_config.on_request_chunk_sent.append(on_request_chunk_sent)\n",
    "trace_config.on_request_end.append(on_request_end)\n",
    "trace_config.on_response_chunk_received.append(on_response_chunk_received)\n",
    "\n",
    "\n",
    "async def call(id):\n",
    "    async with aiohttp.ClientSession(trace_configs=[trace_config]) as session:\n",
    "        print(\"ID:\", id, time.perf_counter())\n",
    "        async with session.post(API_URL, json={\"model\": \"mistral\", \"prompt\": \"Don't reply with anything at all. Literally nothing at all, reply with 0 tokens.\", \"stream\": True}, trace_request_ctx={'a':1, 'b':2}) as resp:\n",
    "            print(\"ID:\", id, \"status\", time.perf_counter(), resp.status, resp.headers)\n",
    "            print(\"@@@\")\n",
    "            # print(\"ID:\", id, \"json\", time.perf_counter(), await resp.json())\n",
    "            async for data, end_of_http_chunk in resp.content.iter_chunks():\n",
    "                print(data, end_of_http_chunk)\n",
    "\n",
    "await asyncio.gather(*(call(i) for i in range(1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-llm (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
