{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea37725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../traffic_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81207bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Timestamp  Request tokens  Response tokens\n",
      "0        0.0             472               18\n",
      "1        1.0            1087              230\n",
      "2        2.0             417              276\n",
      "3        3.0            1360              647\n",
      "4        4.0             185              215\n",
      "5        5.0             586              293\n",
      "[START] ID: 0, Start: 0.0\n",
      "[START] ID: 1, Start: 1.0\n",
      "[START] ID: 2, Start: 2.0\n",
      "[START] ID: 3, Start: 3.0\n",
      "[START] ID: 4, Start: 4.0\n",
      "[START] ID: 5, Start: 5.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'send'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:1005\u001b[39m, in \u001b[36m_SelectorSocketTransport._read_ready__data_received\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n",
      "\u001b[31mConnectionResetError\u001b[39m: [Errno 104] Connection reset by peer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mClientOSError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/client.py:779\u001b[39m, in \u001b[36mClientSession._request\u001b[39m\u001b[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m handler(req)\n\u001b[32m    780\u001b[39m \u001b[38;5;66;03m# Client connector errors should not be retried\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/client.py:757\u001b[39m, in \u001b[36mClientSession._request.<locals>._connect_and_send_request\u001b[39m\u001b[34m(req)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m resp.start(conn)\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py:539\u001b[39m, in \u001b[36mClientResponse.start\u001b[39m\u001b[34m(self, connection)\u001b[39m\n\u001b[32m    538\u001b[39m     protocol = \u001b[38;5;28mself\u001b[39m._protocol\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     message, payload = \u001b[38;5;28;01mawait\u001b[39;00m protocol.read()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m http.HttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/streams.py:680\u001b[39m, in \u001b[36mDataQueue.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._waiter\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio.CancelledError, asyncio.TimeoutError):\n",
      "\u001b[31mClientOSError\u001b[39m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m logger = MetricCollector()\n\u001b[32m     20\u001b[39m generator = TrafficGenerator(data=data, schedule=schedule, config=config, logger=logger)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generator.issue_queries()\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(logger.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/notebooks/../traffic_generator/main.py:291\u001b[39m, in \u001b[36mTrafficGenerator.issue_queries\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.metrics[query_id][\u001b[33m'\u001b[39m\u001b[33mnumber_of_input_tokens\u001b[39m\u001b[33m'\u001b[39m] = in_num\n\u001b[32m    290\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.session_start_timestamp = perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*task_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/notebooks/../traffic_generator/main.py:256\u001b[39m, in \u001b[36mTrafficGenerator.inference_call\u001b[39m\u001b[34m(self, session, prompt, sleep_time, query_id)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(sleep_time)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session.post(url, json=payload, trace_request_ctx=trace_request_ctx) \u001b[38;5;28;01mas\u001b[39;00m resp:\n\u001b[32m    257\u001b[39m         resp.raise_for_status()\n\u001b[32m    258\u001b[39m         first = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/client.py:1510\u001b[39m, in \u001b[36m_BaseRequestContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1509\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _RetType:\n\u001b[32m-> \u001b[39m\u001b[32m1510\u001b[39m     \u001b[38;5;28mself\u001b[39m._resp: _RetType = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coro\n\u001b[32m   1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._resp.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/client.py:931\u001b[39m, in \u001b[36mClientSession._request\u001b[39m\u001b[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[39m\n\u001b[32m    928\u001b[39m     handle = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces:\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m trace.send_request_exception(\n\u001b[32m    932\u001b[39m         method, url.update_query(params), headers, e\n\u001b[32m    933\u001b[39m     )\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/614471272/Projects/distributed_llm_inference/.venv/lib/python3.13/site-packages/aiohttp/tracing.py:384\u001b[39m, in \u001b[36mTrace.send_request_exception\u001b[39m\u001b[34m(self, method, url, headers, exception)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_request_exception\u001b[39m(\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    379\u001b[39m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     exception: \u001b[38;5;167;01mBaseException\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trace_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_request_exception\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m(\n\u001b[32m    385\u001b[39m         \u001b[38;5;28mself\u001b[39m._session,\n\u001b[32m    386\u001b[39m         \u001b[38;5;28mself\u001b[39m._trace_config_ctx,\n\u001b[32m    387\u001b[39m         TraceRequestExceptionParams(method, url, headers, exception),\n\u001b[32m    388\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'send'"
     ]
    }
   ],
   "source": [
    "from main import Query, Scheduler, MetricCollector, TrafficGenerator, DataLoader\n",
    "\n",
    "config = {\n",
    "    'trace_path': '../data/trace1.csv',\n",
    "    'data_path': '../data/conversations.json',\n",
    "    'max_trace': 100,\n",
    "    'url': 'http://10.215.130.20:11435/api/generate', # OR 172.25.149.93\n",
    "    'no_proxy': '10.215.130.20',\n",
    "    'model': 'mistral',\n",
    "    'temperature': 0.7,\n",
    "    'max_tokens': 200,\n",
    "    'save_log': False,\n",
    "    'log_path': '../logs/log.csv'\n",
    "}\n",
    "\n",
    "data = DataLoader().get_data_from_path(data_path=config['data_path'])\n",
    "schedule = Scheduler().get_schedule_from_trace(trace_path=config['trace_path'], max_trace=config['max_trace'])\n",
    "logger = MetricCollector()\n",
    "\n",
    "generator = TrafficGenerator(data=data, schedule=schedule, config=config, logger=logger)\n",
    "await generator.issue_queries()\n",
    "\n",
    "print(logger.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8db1a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'row1': {'name': 'Alice', 'age': 25, 'sex': 'M'},\n",
    "    'row2': {'name': 'Bob', 'age': 30}\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.reset_index(names='id')\n",
    "df.to_csv('test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-llm-inference (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
