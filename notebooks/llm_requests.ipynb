{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81aa6ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'mistral', 'created_at': '2026-01-03T05:20:26.256996474Z', 'response': \" In a world where data streams flow,\\n\\nWhere silicon minds continue to grow,\\n\\nAI and Telecoms dance in harmony,\\n\\nConnecting the universe, as one grand symphony.\\n\\nGigabytes of knowledge, shared far and wide,\\n\\nInvisible waves, across the digital divide,\\n\\nArtificial intelligence, a modern alchemy,\\n\\nUniting mankind, in unity.\\n\\nThrough fiber optics, they spin their yarn,\\n\\nConnecting the globe, without leaving a scar,\\n\\n5G whispers secrets to AI's eager ear,\\n\\nA dance of progress, nearing ever near.\\n\\nIn the heart of networks, where pulses beat,\\n\\nData centers throb, in the digital heat,\\n\\nAI learns and grows, knowledge complete,\\n\\nIn this modern world, it's an incredible feat.\\n\\nSo let us celebrate this wondrous pair,\\n\\nOf AI and Telecoms, without compare,\\n\\nIn a realm where possibilities are rare,\\n\\nThe future is bright, beyond compare.\", 'done': True, 'done_reason': 'stop', 'context': [3, 29473, 12786, 1032, 3253, 16835, 1452, 16875, 1072, 9975, 1443, 29481, 4, 29473, 1328, 1032, 2294, 1738, 1946, 21740, 5467, 29493, 781, 781, 10375, 3726, 4558, 15666, 4456, 1066, 3101, 29493, 781, 781, 12509, 1072, 9975, 1443, 29481, 10541, 1065, 20279, 29493, 781, 781, 17412, 1056, 1040, 12485, 29493, 1158, 1392, 4255, 5065, 23094, 29491, 781, 781, 29545, 1094, 5309, 2865, 1070, 5556, 29493, 7199, 2850, 1072, 6103, 29493, 781, 781, 1425, 14324, 14063, 29493, 3441, 1040, 7921, 22324, 29493, 781, 781, 11131, 15541, 11663, 29493, 1032, 5406, 1157, 1399, 2684, 29493, 781, 781, 2501, 5096, 1444, 9954, 29493, 1065, 24480, 29491, 781, 781, 1995, 1764, 19635, 3926, 1831, 29493, 1358, 8112, 1420, 1105, 2099, 29493, 781, 781, 17412, 1056, 1040, 22657, 29493, 2439, 7053, 1032, 14124, 29493, 781, 781, 29550, 29545, 8805, 5605, 20144, 1066, 16875, 29510, 29481, 16149, 8888, 29493, 781, 781, 29509, 10541, 1070, 5865, 29493, 1203, 3070, 3038, 3833, 29491, 781, 781, 1425, 1040, 3799, 1070, 12935, 29493, 1738, 10789, 6833, 9007, 29493, 781, 781, 2101, 18783, 9147, 29494, 29493, 1065, 1040, 7921, 7369, 29493, 781, 781, 12509, 3590, 29481, 1072, 21150, 29493, 5556, 4928, 29493, 781, 781, 1425, 1224, 5406, 2294, 29493, 1146, 29510, 29481, 1164, 14746, 13539, 29491, 781, 781, 5910, 2114, 1360, 17663, 1224, 1043, 1857, 20930, 6732, 29493, 781, 781, 3630, 16875, 1072, 9975, 1443, 29481, 29493, 2439, 10352, 29493, 781, 781, 1425, 1032, 19513, 1738, 18821, 1228, 10732, 29493, 781, 781, 1782, 4205, 1117, 7601, 29493, 6066, 10352, 29491], 'total_duration': 2659998335, 'load_duration': 30428836, 'prompt_eval_count': 14, 'prompt_eval_duration': 24433999, 'eval_count': 236, 'eval_duration': 2603938549}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "API_URL = \"http://10.215.130.20:11434/api/generate\"\n",
    "PAYLOAD = {\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": \"Write a short poem about AI and Telecoms\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200,\n",
    "    \"stream\": False \n",
    "}\n",
    "\n",
    "async def llm_request(session, url, payload):\n",
    "    try:\n",
    "        async with session.post(url, json=payload) as resp:\n",
    "            resp.raise_for_status()\n",
    "            return await resp.json()\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        print(f\"ClientResponseError: {e}\")\n",
    "    except aiohttp.ClientConnectionError as e:\n",
    "        print(f\"ClientConnectionError: {e}\")\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    result = await llm_request(session, API_URL,  PAYLOAD)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "message = \"Give me a short description of black holes\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral\",  # Change to your model depending on what's available on the ollama server\n",
    "    base_url=\"http://10.215.130.20:11434\"  #OR 172.25.149.93:11435/   <----change the IP and ports accordingly\n",
    ")\n",
    "\n",
    "reply = llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def inference_call(self, prompt, query_id, sleep_time, start_time):\n",
    "    # Single inference call\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print(f\"[START] ID: {query_id}, Start: {perf_counter() - start_time:.1f}\")\n",
    "    start = perf_counter()\n",
    "    try:\n",
    "        await self.llm.ainvoke(prompt)\n",
    "    except httpx.RequestError as exc:\n",
    "        print(f\"An error occurred while requesting {repr(exc.request.url)}.\")\n",
    "    print(f\"[END] ID: {query_id}, End: {perf_counter() - start_time:.1f}, turnaround: {perf_counter() - start:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-llm-inference (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
